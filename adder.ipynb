{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains a GPT to add n-digit numbers.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "from mingpt.utils import set_seed, setup_logging, CfgNode as CN\n",
    "\n",
    "from nndrain.tensor_edit import TensorEdit\n",
    "from nndrain.simplify_linear import SimplifyLinear\n",
    "import nndrain.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates n-digit addition problems. For example, if n=2, then an example\n",
    "    addition problem would be to add 85 + 50 = 135. This problem would be\n",
    "    represented as the following string for the GPT:\n",
    "\n",
    "    \"8550531\"\n",
    "\n",
    "    This is because:\n",
    "    - we are discarding the + and =, which are not necessary. We just encode the digits\n",
    "      of the input numbers concatenated together.\n",
    "    - the result 135 is encoded backwards to make the addition easier to learn for the\n",
    "      GPT model, because of how the addition algorithm works.\n",
    "\n",
    "    As one more example, the problem 6 + 39 = 45 would be encoded as:\n",
    "\n",
    "    \"0639054\"\n",
    "\n",
    "    where you will notice that we are padding with zeros to make sure that we always\n",
    "    produce strings of the exact same size: n + n + (n + 1). When n=2, this is 7.\n",
    "    At test time, we will feed in an addition problem by giving the first 2n digits,\n",
    "    and hoping that the GPT model completes the sequence with the next (n+1) digits\n",
    "    correctly.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CN()\n",
    "        C.ndigit = 2\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config, split):\n",
    "        self.config = config\n",
    "        self.split = split # train/test\n",
    "\n",
    "        # split up all addition problems into either training data or test data\n",
    "        ndigit = self.config.ndigit\n",
    "        assert ndigit <= 3, \"the lines below would be very memory inefficient, in future maybe refactor to support\"\n",
    "        num = (10**ndigit)**2 # total number of possible addition problems with ndigit numbers\n",
    "        rng = torch.Generator()\n",
    "        rng.manual_seed(1337)\n",
    "        perm = torch.randperm(num, generator=rng)\n",
    "        num_test = min(int(num*0.2), 500) # 20% of the whole dataset, or only up to 500\n",
    "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return 10 # digits 0..9\n",
    "\n",
    "    def get_block_size(self):\n",
    "        # a,b,a+b, and +1 due to potential carry overflow,\n",
    "        # but then also -1 because very last digit doesn't ever plug back\n",
    "        # as there is no explicit <EOS> token to predict, it is implied\n",
    "        return 3*self.config.ndigit + 1 - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ixes.nelement()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ndigit = self.config.ndigit\n",
    "        # given a problem index idx, first recover the associated a + b\n",
    "        idx = self.ixes[idx].item()\n",
    "        nd = 10**ndigit\n",
    "        a = idx // nd\n",
    "        b = idx %  nd\n",
    "        # calculate the \"label\" of the addition problem a + b\n",
    "        c = a + b\n",
    "        # encode the digits of a, b, c into strings\n",
    "        astr = f'%0{ndigit}d' % a\n",
    "        bstr = f'%0{ndigit}d' % b\n",
    "        cstr = (f'%0{ndigit+1}d' % c)[::-1] # reverse c to make addition easier\n",
    "        render = astr + bstr + cstr\n",
    "        dix = [int(s) for s in render] # convert each character to its token index\n",
    "        # x will be input to GPT and y will be the associated expected outputs\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
    "        y[:ndigit*2-1] = -1 # we will only train in the output locations. -1 will mask loss to zero\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "\n",
    "    C = CN()\n",
    "\n",
    "    # system\n",
    "    C.system = CN()\n",
    "    C.system.seed = 3407\n",
    "    C.system.work_dir = './out/adder'\n",
    "\n",
    "    # data\n",
    "    C.data = AdditionDataset.get_default_config()\n",
    "    C.data.ndigit=2\n",
    "\n",
    "    # model\n",
    "    C.model = GPT.get_default_config()\n",
    "    C.model.model_type = 'gpt-micro'\n",
    "\n",
    "    # trainer\n",
    "    C.trainer = Trainer.get_default_config()\n",
    "    C.trainer.num_workers = 0\n",
    "    C.trainer.learning_rate = 6e-4 # the model we're using is so small that we can go a bit faster\n",
    "    C.trainer.batch_size = 512\n",
    "    C.trainer.max_iters = 50000\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "if __name__ == '__main__':\n",
    "    # get default config and overrides from the command line, if any\n",
    "    config = get_config()\n",
    "\n",
    "    print(config)\n",
    "    setup_logging(config)\n",
    "    set_seed(config.system.seed)\n",
    "\n",
    "    # construct train and test datasets\n",
    "    train_dataset = AdditionDataset(config.data, split='train')\n",
    "    test_dataset  = AdditionDataset(config.data, split='test')\n",
    "\n",
    "    # construct the model\n",
    "    config.model.vocab_size = train_dataset.get_vocab_size()\n",
    "    config.model.block_size = train_dataset.get_block_size()\n",
    "    model = GPT(config.model)\n",
    "\n",
    "    ####################################\n",
    "    n_start_params = sum(p.numel() for p in model.parameters())\n",
    "    simplify_layers = [module for module in model.modules() if isinstance(module, SimplifyLinear)]\n",
    "    te = TensorEdit(simplify_layers)\n",
    "    drain_threshold_coeff = 3.0\n",
    "    remove_threshold_coeff = 0.1\n",
    "    fig, fig_chart = None, None\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(figsize=(16, 10), facecolor='white')    \n",
    "    filenames = []\n",
    "    outdir = 'out/adder'\n",
    "    ####################################\n",
    "\n",
    "    # construct the trainer object\n",
    "    trainer = Trainer(config.trainer, model, train_dataset)\n",
    "\n",
    "    # helper function for the evaluation of a model\n",
    "    def eval_split(trainer, split, max_batches=None):\n",
    "        dataset = {'train':train_dataset, 'test':test_dataset}[split]\n",
    "        ndigit = config.data.ndigit\n",
    "        results = []\n",
    "        mistakes_printed_already = 0\n",
    "        factors = torch.tensor([[10**i for i in range(ndigit+1)][::-1]]).to(trainer.device)\n",
    "        loader = DataLoader(dataset, batch_size=100, num_workers=0, drop_last=False)\n",
    "        for b, (x, y) in enumerate(loader):\n",
    "            x = x.to(trainer.device)\n",
    "            # isolate the first two digits of the input sequence alone\n",
    "            d1d2 = x[:, :ndigit*2]\n",
    "            # let the model sample the rest of the sequence\n",
    "            d1d2d3 = model.generate(d1d2, ndigit+1, do_sample=False) # using greedy argmax, not sampling\n",
    "            # isolate the last digit of the sampled sequence\n",
    "            d3 = d1d2d3[:, -(ndigit+1):]\n",
    "            d3 = d3.flip(1) # reverse the digits to their \"normal\" order\n",
    "            # decode the integers from individual digits\n",
    "            d1i = (d1d2[:,:ndigit] * factors[:,1:]).sum(1)\n",
    "            d2i = (d1d2[:,ndigit:ndigit*2] * factors[:,1:]).sum(1)\n",
    "            d3i_pred = (d3 * factors).sum(1)\n",
    "            d3i_gt = d1i + d2i # manually calculate the ground truth\n",
    "            # evaluate the correctness of the results in this batch\n",
    "            correct = (d3i_pred == d3i_gt).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line haha\n",
    "            for i in range(x.size(0)):\n",
    "                results.append(int(correct[i]))\n",
    "                if not correct[i] and mistakes_printed_already < 5: # only print up to 5 mistakes to get a sense\n",
    "                    mistakes_printed_already += 1\n",
    "                    print(\"GPT claims that %d + %d = %d but gt is %d\" % (d1i[i], d2i[i], d3i_pred[i], d3i_gt[i]))\n",
    "            if max_batches is not None and b+1 >= max_batches:\n",
    "                break\n",
    "        rt = torch.tensor(results, dtype=torch.float)\n",
    "        score_print = \"%s final score: %d/%d = %.2f%% correct\" % (split, rt.sum(), len(results), 100*rt.mean())\n",
    "        print(score_print)\n",
    "        return rt.sum(), score_print\n",
    "\n",
    "\n",
    "    # iteration callback\n",
    "    top_score = 0\n",
    "    def batch_end_callback(trainer):\n",
    "        global top_score\n",
    "\n",
    "        epoch = int(trainer.iter_num/150) #define an arbitrary epoch just to periodically execute the underlying methods\n",
    "        te.weights_drain(p_drain=0.1, threshold_coeff=drain_threshold_coeff, condition=epoch>20 and epoch%2==0)\n",
    "        te.weights_decay(p_decay=0.1, decay_rate=5e-3, condition=epoch>20 and epoch%2==0)\n",
    "\n",
    "        removed = False\n",
    "        if trainer.iter_num % 10 == 0:\n",
    "            print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "\n",
    "            # remove weights if all values ​​in a row or column are less than the specified value\n",
    "            if te.weights_remove(p_remove=1, threshold_coeff=remove_threshold_coeff, max_removal=1.0, verbose=True):\n",
    "                # re-instantiate the optimizer with the new model if I have deleted any rows or columns\n",
    "                trainer.optimizer = model.configure_optimizers(config.trainer)\n",
    "\n",
    "\n",
    "        if trainer.iter_num % 250 == 0:\n",
    "\n",
    "            # evaluate both the train and test score\n",
    "            train_max_batches = {1: None, 2: None, 3: 5, 4: 5}[config.data.ndigit] # if ndigit=2 we can afford the whole train set, ow no\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_score, title3 = eval_split(trainer, 'train', max_batches=train_max_batches)\n",
    "                test_score, title4  = eval_split(trainer, 'test',  max_batches=None)\n",
    "            score = train_score + test_score\n",
    "            # save the model if this is the best score we've seen so far\n",
    "            if score > top_score:\n",
    "                top_score = score\n",
    "                print(f\"saving model with new top score of {score}\")\n",
    "                ckpt_path = os.path.join(config.system.work_dir, \"model.pt\")\n",
    "                torch.save(model.state_dict(), ckpt_path)\n",
    "            # revert model to training mode\n",
    "            model.train()\n",
    "\n",
    "            # plot\n",
    "            n_params = sum(p.numel() for p in model.parameters())\n",
    "            simplification = (1-n_params/n_start_params)*100\n",
    "            print(\"number of parameters: {:.3e} simplification: {:.1f}%\".format(n_params, simplification))\n",
    "\n",
    "            weights = [w.weight.data.T if w.weight.data.size()[1]>w.weight.data.size()[0] else w.weight.data for w in simplify_layers]\n",
    "\n",
    "            title1 = \"minGPT Adder Example Iter [{}/{}] Train Loss: {:.5f} Params/StartParams: {}/{}\"\\\n",
    "                        .format(trainer.iter_num, config.trainer.max_iters, trainer.loss.item(), n_params, n_start_params)\n",
    "            title2 = \"Linear Layer Weights Simplification: {:.1f}% Iter time: {:.2f}ms\"\\\n",
    "                        .format(simplification, trainer.iter_dt * 1000)\n",
    "            ax_title = []\n",
    "            for i_w, w in enumerate(weights):\n",
    "                if simplify_layers[i_w].threshold is not None:\n",
    "                    v_min_row, i_min_row = te.get_min_weight_row_threshold(w)\n",
    "                    v_min_col, i_min_col = te.get_min_weight_col_threshold(w)\n",
    "                    v_max_row, i_max_row = te.get_max_weight_row_threshold(w)\n",
    "                    layer_remove_threshold = remove_threshold_coeff*simplify_layers[i_w].threshold\n",
    "                    ax_title.append(\"W{}\\nthreshold {:.4f}\\nrow_min {},{:.4f}\\ncol_min {},{:.4f}\\nmax {},{:.4f}\\n\".format(i_w, layer_remove_threshold, i_min_row, v_min_row, i_min_col, v_min_col, i_max_row, v_max_row))\n",
    "\n",
    "            utils.plot_weights(fig, weights, '\\n'.join((title1, title2, title3, title4)), ax_title=ax_title if ax_title!=[] else None)\n",
    "            \n",
    "            # create a file name, append it to the filenames list\n",
    "            filename = '/'.join((outdir, f'frame_{epoch+1}.png'))\n",
    "            filenames.append(filename)\n",
    "            # and save the figure\n",
    "            fig.savefig(filename, facecolor=fig.get_facecolor())\n",
    "            plt.clf()\n",
    "                \n",
    "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "    # run the optimization\n",
    "    trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a gif by composing the frames\n",
    "utils.images_to_gif(filenames, '/'.join((outdir, 'adder.gif')), tail=100)\n",
    "\n",
    "# create a mjpg video\n",
    "utils.images_to_avi(filenames, '/'.join((outdir, 'adder.avi')))\n",
    "\n",
    "# Remove frame files\n",
    "for filename in set(filenames):\n",
    "    os.remove(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('3.9.2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "245dbc792dc39c3f0a9aacf957a37e7ff4bdace881e9e1e4748b1efe00e527b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
