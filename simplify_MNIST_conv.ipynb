{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from nndrain.tensor_edit import TensorEdit\n",
    "from nndrain.simplify_linear import SimplifyLinear\n",
    "import nndrain.utils as utils\n",
    "\n",
    "\n",
    "utils.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc1 = SimplifyLinear(7*7*32, 7*7*32, True, False, False)\n",
    "        self.fc2 = SimplifyLinear(7*7*32, num_classes, False, True, False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.activation(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Define Hyper parameters\n",
    "num_classes = 10\n",
    "model = ConvNet(num_classes).to(device)\n",
    "n_start_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 250\n",
    "fig = None\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "\n",
    "filenames = []\n",
    "outdir = 'out/mnist/conv'\n",
    "isExist = os.path.exists(outdir)\n",
    "if not isExist:\n",
    "    os.makedirs(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "# Loss and optimizer\n",
    "lr = 1e-3\n",
    "lossFun = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# weights figure\n",
    "fig = plt.figure(figsize=(16, 10), facecolor='white')\n",
    "# select the layers that can be simplified\n",
    "simplify_layers = [module for module in model.modules() if isinstance(module, SimplifyLinear)]\n",
    "te = TensorEdit(simplify_layers)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    samples = 0\n",
    "    correct = 0\n",
    "\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = lossFun(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        samples += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # the force pulling down the weights in the range [-r_value; r_value] to zero\n",
    "        te.weights_drain(p_drain=0.005, threshold_coeff=3.0, condition=epoch!=0 and epoch%5==0)\n",
    "        # the following decay avoids getting stuck in the simplification process.\n",
    "        te.weights_decay(p_decay=0.01, decay_rate=5e-3, condition=epoch!=0 and epoch%2==0)\n",
    "        \n",
    "    # remove weights if all values ​​in a row or column are less than the specified value\n",
    "    if te.weights_remove(p_remove=1, threshold_coeff=0.5, max_removal=0.1, verbose=True):\n",
    "        # re-instantiate the optimizer with the new model if I have deleted any rows or columns\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    # train loss, accuracy, number of parameters\n",
    "    train_accuracy = 100 * correct / samples\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_losses.append(train_loss)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    simplification = (1 - n_params / n_start_params) * 100\n",
    "\n",
    "    # test the model\n",
    "    model.eval()\n",
    "    test_accuracy = 0\n",
    "    test_dt = 0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        test_samples = 0\n",
    "        tnow = time.time()\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_samples += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        test_dt = (time.time() - tnow) / test_samples\n",
    "        test_accuracy = 100 * correct / test_samples\n",
    "\n",
    "    # select the weight matrices transposing them for a better visualization\n",
    "    weights = [w.weight.data.T for w in simplify_layers]\n",
    "    title1 = \"MNIST Example Epoch [{}/{}] Train Loss: {:.4f} Params/StartParams: {}/{}\"\\\n",
    "                .format(epoch+1, num_epochs, loss.item(), n_params, n_start_params)\n",
    "    title2 = \"Params Simplification: {:.1f}% Inference time: {:.2f}ms\"\\\n",
    "                .format(simplification, test_dt*1000)\n",
    "    title3 = \"Train Accuracy: {:.2f}% Test Accuracy: {:.2f}%\"\\\n",
    "                .format(train_accuracy, test_accuracy)\n",
    "    utils.plot_weights(fig, weights, '\\n'.join((title1, title2, title3)))\n",
    "    \n",
    "    # create a file name, append it to the filenames list\n",
    "    filename = '/'.join((outdir, f'frame_{epoch+1}.png'))\n",
    "    filenames.append(filename)\n",
    "    # and save the figure\n",
    "    fig.savefig(filename, facecolor=fig.get_facecolor())\n",
    "    plt.clf()\n",
    "\n",
    "    print(' '.join((title1, title2, title3)))\n",
    "\n",
    "    if simplification > 95.0 and test_accuracy > 98.0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a gif by composing the frames\n",
    "utils.images_to_gif(filenames, '/'.join((outdir, 'mnist_conv.gif')), tail=100)\n",
    "\n",
    "# create a mjpg video\n",
    "utils.images_to_avi(filenames, '/'.join((outdir, 'mnist_conv.avi')))\n",
    "\n",
    "# Remove frame files\n",
    "for filename in set(filenames):\n",
    "    os.remove(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('3.9.2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "245dbc792dc39c3f0a9aacf957a37e7ff4bdace881e9e1e4748b1efe00e527b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
